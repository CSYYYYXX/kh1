{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda19887",
   "metadata": {},
   "source": [
    "\n",
    "# CAE-Transformer Reduced Order Model——Kelvin–Helmholtz instability problem\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In order to effectively reduce the design cost and cycle time of using CFD methods, the reduced-order model (ROM) has gained wide attention in recent years. For complex compressible flows, using linear methods such as Proper Orthogonal Decomposition (POD) for flow field dimensionality reduction requires a large number of modes to ensure the reconstruction accuracy. It has been shown that the modes number can be effectively reduced by using nonlinear dimensionality reduction methods. Convolutional Autoencoder (CAE) is a kind of neural network composed of encoder and decoder, which can realize data dimensionality reduction and recon-struction, and can be regarded as a nonlinear extension of POD method. CAE is used for nonlinear dimension-ality reduction, and Transformer is used for time evolution. The CAE-Transformer can obtain high reconstruction and prediction accuracy on the premise of using less latents for unsteady compressible flows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39552eaf",
   "metadata": {},
   "source": [
    "## Framework of CAE-Transformer\n",
    "\n",
    "The CAE-Transformer reduced order model uses a CAE network to reduce the dimensionality of the flow field, extract the characteristics of the flow data, compress it into the hidden space of the encoder, and then use the Transformer network to perform time evolution through the mechanism of self-attention on the free variables in the hidden space to obtain the free variables at other times of flow. Then, the decoder of the CAE network decodes the evolved free variables and reconstructs the flow field flow data at the corresponding time. The construction of the CAE-Transformer flow reduction model relies on the data reduction of the CAE network and the time evolution of the Transformer network. Compared with existing methods such as POD/DMD, using CAE networks for nonlinear dimensionality reduction of flow field data and Transformer networks for equation free evolution of free variables can achieve higher compression ratios and improve the efficiency of flow field prediction while ensuring the accuracy of the flow field reduction model.\n",
    "\n",
    "+ Input：Input the flow field for a period of time.\n",
    "+ Compression：Extract high-dimensional spatiotemporal flow characteristics by dimensionality reduction of the flow field using the encoder of CAE.\n",
    "+ Evolution：Learning the evolution of spatiotemporal characteristics of low dimensional spatial flow fields through Transformer and predicting the next moment.\n",
    "+ Reconstruction：Restore the predicted low-dimensional features of the flow field to high-dimensional space through the decoder of CAE.\n",
    "+ Output：Output the predicted results of the transient flow field at the next moment.\n",
    "\n",
    "The first step is to train the CAE network. After the training is completed, the CAE encoder is used to obtain the low dimensional features of the flow field. This low dimensional feature is used as the dataset of the Transformer network for Transformer network training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901d5c41",
   "metadata": {},
   "source": [
    "![CAE_Transformer.png](./images/CAE_Transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7406dd",
   "metadata": {},
   "source": [
    "## Training environment\n",
    "\n",
    "Import the required function library for training, where src includes dataset creation functions, network models, and training loss visualization functions.\n",
    "\n",
    "The static GRAPH of Mindspore framework is adopted for training. Training can be done on GPU(default) or Ascend(single card)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17230db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from mindspore import nn, ops, context, save_checkpoint, set_seed, jit, data_sink\n",
    "from mindflow.utils import load_yaml_config\n",
    "\n",
    "from src import create_cae_dataset, CaeNet, plot_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e3ba84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa53aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='cae net for kh')\n",
    "parser.add_argument(\"--mode\", type=str, default=\"GRAPH\", choices=[\"GRAPH\", \"PYNATIVE\"],\n",
    "                    help=\"Context mode, support 'GRAPH', 'PYNATIVE'\")\n",
    "parser.add_argument(\"--save_graphs\", type=bool, default=False, choices=[True, False],\n",
    "                    help=\"Whether to save intermediate compilation graphs\")\n",
    "parser.add_argument(\"--save_graphs_path\", type=str, default=\"./graphs\")\n",
    "parser.add_argument(\"--device_target\", type=str, default=\"GPU\", choices=[\"GPU\", \"Ascend\"],\n",
    "                    help=\"The target device to run, support 'Ascend', 'GPU'\")\n",
    "parser.add_argument(\"--device_id\", type=int, default=0, help=\"ID of the target device\")\n",
    "parser.add_argument(\"--config_file_path\", type=str, default=\"./config.yaml\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE if args.mode.upper().startswith(\"GRAPH\") else context.PYNATIVE_MODE,\n",
    "                    save_graphs=args.save_graphs,\n",
    "                    save_graphs_path=args.save_graphs_path,\n",
    "                    device_target=args.device_target,\n",
    "                    device_id=args.device_id)\n",
    "use_ascend = context.get_context(attr_key='device_target') == \"Ascend\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd5ca2c",
   "metadata": {},
   "source": [
    "## CAE training parameter settings\n",
    "\n",
    "Import parameter configurations for the dataset, CAE model, and optimizer from the config.yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e0f61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml_config(args.config_file_path)\n",
    "data_params = config[\"cae_data\"]\n",
    "model_params = config[\"cae_model\"]\n",
    "optimizer_params = config[\"cae_optimizer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e53d5ec",
   "metadata": {},
   "source": [
    "The default path for saving loss files during training is optimizer_params [\"summary_dir\"], the weight parameters are saved in the ckpt folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e34bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dir = optimizer_params[\"summary_dir\"]\n",
    "if not os.path.exists(summary_dir):\n",
    "    os.mkdir(summary_dir)\n",
    "ckpt_dir = os.path.join(summary_dir, 'ckpt')\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.mkdir(ckpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505908fc",
   "metadata": {},
   "source": [
    "## Construct CAE neural network\n",
    "\n",
    "The CAE network consists of six layers of convolution and maximum pooling to form an encoder, and seven layers of convolution and upsampling to form a decoder. Use MSELoss loss function and Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbe1356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cae = CaeNet(model_params[\"data_dimension\"], model_params[\"conv_kernel_size\"], model_params[\"maxpool_kernel_size\"],\n",
    "             model_params[\"maxpool_stride\"], model_params[\"encoder_channels\"], model_params[\"decoder_channels\"],\n",
    "             model_params[\"channels_dense\"])\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "cae_opt = nn.Adam(cae.trainable_params(), optimizer_params[\"lr\"], weight_decay=optimizer_params[\"weight_decay\"])\n",
    "\n",
    "if use_ascend:\n",
    "    from mindspore.amp import DynamicLossScaler, auto_mixed_precision, all_finite\n",
    "    loss_scaler = DynamicLossScaler(1024, 2, 100)\n",
    "    auto_mixed_precision(cae, 'O1')\n",
    "else:\n",
    "    loss_scaler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea2da0",
   "metadata": {},
   "source": [
    "## CAE dataset\n",
    "\n",
    "Dataset download address: data_driven/cae-lstm/kh/dataset\n",
    "\n",
    "The dataset in this case is the numerical simulation results of a two-dimensional Kelvin-Helmholtz instability problem, the instability in parallel shear flow is called KH instability. The range of coordinates x and y is \\[-0.5, 0.5\\], and the time t range is \\[0, 1.5\\]. A total of 1786 flow field snapshots, each with a matrix size of (256, 256).\n",
    "\n",
    "After importing the dataset, perform data sinking settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb8487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cae_dataset, _ = create_cae_dataset(data_params[\"data_path\"], data_params[\"batch_size\"])\n",
    "\n",
    "sink_process = data_sink(train_step, cae_dataset, sink_size=1)\n",
    "train_data_size = cae_dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771dfcdf",
   "metadata": {},
   "source": [
    "## CAE training\n",
    "\n",
    "Build forward_fn and train_step, start training the CAE network and visualize the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f16d65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid:14003\n",
      "====================Start cae train=======================\n",
      "epoch: 1 train loss: 0.16278297 epoch time: 16.27s\n",
      "epoch: 2 train loss: 0.098297514 epoch time: 2.26s\n",
      "epoch: 3 train loss: 0.07453717 epoch time: 2.26s\n",
      "epoch: 4 train loss: 0.05944114 epoch time: 2.27s\n",
      "epoch: 5 train loss: 0.050014462 epoch time: 2.25s\n",
      "...\n",
      "epoch: 4396 train loss: 0.0009265681 epoch time: 2.23s\n",
      "epoch: 4397 train loss: 0.00071872084 epoch time: 2.22s\n",
      "epoch: 4398 train loss: 0.00075864815 epoch time: 2.22s\n",
      "epoch: 4399 train loss: 0.00073658983 epoch time: 2.22\n",
      "epoch: 4400 train loss: 0.0006535899 epoch time: 2.22s\n",
      "====================End cae train=======================\n"
     ]
    }
   ],
   "source": [
    "def forward_fn(data, label):\n",
    "    logits = cae(data)\n",
    "    loss = loss_fn(logits, label)\n",
    "    if use_ascend:\n",
    "        loss = loss_scaler.scale(loss)\n",
    "    return loss\n",
    "\n",
    "grad_fn = ops.value_and_grad(forward_fn, None, cae_opt.parameters, has_aux=False)\n",
    "\n",
    "@jit\n",
    "def train_step(data, label):\n",
    "    loss, grads = grad_fn(data, label)\n",
    "    if use_ascend:\n",
    "        loss = loss_scaler.unscale(loss)\n",
    "        if all_finite(grads):\n",
    "            grads = loss_scaler.unscale(grads)\n",
    "    loss = ops.depend(loss, cae_opt(grads))\n",
    "    return loss\n",
    "\n",
    "print(f\"====================Start cae train=======================\")\n",
    "train_loss = []\n",
    "for epoch in range(1, optimizer_params[\"epochs\"] + 1):\n",
    "    local_time_beg = time.time()\n",
    "    cae.set_train()\n",
    "    epoch_train_loss = 0\n",
    "    for _ in range(train_data_size):\n",
    "        epoch_train_loss = ops.squeeze(sink_process(), axis=())\n",
    "    train_loss.append(epoch_train_loss)\n",
    "    print(f\"epoch: {epoch} train loss: {epoch_train_loss} epoch time: {time.time() - local_time_beg:.2f}s\")\n",
    "\n",
    "    if epoch % optimizer_params[\"save_ckpt_interval\"] == 0:\n",
    "        save_checkpoint(cae, f\"{ckpt_dir}/cae_{epoch}.ckpt\")\n",
    "print(f\"=====================End cae train========================\")\n",
    "plot_train_loss(train_loss, summary_dir, optimizer_params[\"epochs\"], \"cae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a43727",
   "metadata": {},
   "source": [
    "## CAE flow field reconstruction results\n",
    "\n",
    "After training the CAE network, run cae_prediction.py to view the training results of CAE to determine whether to continue training the LSTM network\n",
    "\n",
    "The following figures show the real flow field, CAE flow field reconstruction results, and the error curves between them. The first two flow field results show the variation of density at different positions in the flow field over time, while the third error curve shows the average relative error of the CAE reconstructed flow field and the real flow field label over time. The error remains below 0.015 for most of the time, meeting the accuracy requirements for flow field reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c7619",
   "metadata": {},
   "source": [
    "<figure class=\"harf\">\n",
    "    <img src=\"./images/true.gif\" title=\"cae_train_loss\" width=\"300\"/>\n",
    "    <img src=\"./images/cae.gif\" title=\"cae_prediction\" width=\"300\"/>\n",
    "    <img src=\"./images/cae_error.png\" title=\"cae_error\" width=\"300\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer training environment\n",
    "Import the library required for training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore as ms\n",
    "\n",
    "from mindspore import nn, ops, context, save_checkpoint, set_seed, jit, data_sink\n",
    "from mindspore.train import CheckpointConfig, ModelCheckpoint\n",
    "from mindvision.engine.callback import LossMonitor\n",
    "\n",
    "from mindflow.utils import load_yaml_config\n",
    "from sympy import Q\n",
    "from src import create_transformer_dataset, plot_train_loss, Informer\n",
    "from cae_prediction import cae_prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "set_seed(42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"transformer net for KH\")\n",
    "parser.add_argument(\n",
    "    \"--mode\",\n",
    "    type=str,\n",
    "    default=\"GRAPH\",\n",
    "    choices=[\"GRAPH\", \"PYNATIVE\"],\n",
    "    help=\"Context mode, support 'GRAPH', 'PYNATIVE'\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_graphs\",\n",
    "    type=bool,\n",
    "    default=False,\n",
    "    choices=[True, False],\n",
    "    help=\"Whether to save intermediate compilation graphs\",\n",
    ")\n",
    "parser.add_argument(\"--save_graphs_path\", type=str, default=\"./graphs\")\n",
    "parser.add_argument(\n",
    "    \"--device_target\",\n",
    "    type=str,\n",
    "    default=\"GPU\",\n",
    "    choices=[\"GPU\", \"Ascend\"],\n",
    "    help=\"The target device to run, support 'Ascend', 'GPU'\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--device_id\", type=int, default=0, help=\"ID of the target device\"\n",
    ")\n",
    "parser.add_argument(\"--config_file_path\", type=str, default=\"./config.yaml\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "ms.set_context(device_target=\"GPU\")\n",
    "ms.set_context(mode=ms.PYNATIVE_MODE)\n",
    "use_ascend = context.get_context(attr_key=\"device_target\") == \"Ascend\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer loss function Settings\n",
    "\n",
    "CustomWithLossCell is a custom loss calculation handling loss function in MindSpore."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomWithLossCell(nn.Cell):\n",
    "    def __init__(self, backbone, loss_fn, args):\n",
    "        super(CustomWithLossCell, self).__init__(auto_prefix=False)\n",
    "        self._backbone = backbone\n",
    "        self._loss_fn = loss_fn\n",
    "        self.args = args\n",
    "\n",
    "    def construct(self, seq_x, seq_y):\n",
    "        cast = ops.Cast()\n",
    "\n",
    "        batch_x = cast(seq_x, ms.float32)\n",
    "        batch_y = cast(seq_y, ms.float32)\n",
    "\n",
    "        if self.args[\"padding\"] == 0:\n",
    "            dec_inp = ops.Zeros()(\n",
    "                (batch_y.shape[0], self.args[\"pred_len\"], batch_y.shape[-1]), ms.float32\n",
    "            )\n",
    "        else:\n",
    "            dec_inp = ops.Ones()(\n",
    "                (batch_y.shape[0], self.args[\"pred_len\"], batch_y.shape[-1]), ms.float32\n",
    "            )\n",
    "        dec_inp = cast(\n",
    "            ops.concat([batch_x[:, - self.args[\"label_len\"] : , :], dec_inp], axis=1),\n",
    "            ms.float32,\n",
    "        )\n",
    "\n",
    "        outputs = self._backbone(batch_x, dec_inp)\n",
    "        batch_y = batch_y[:, -self.args[\"pred_len\"] :, :]\n",
    "\n",
    "        return self._loss_fn(outputs, batch_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "a72e4826",
   "metadata": {},
   "source": [
    "## Transformer network framework and training Settings\n",
    "\n",
    "Start by importing the Transformer model dataset setup parameters, Transformer model and optimizer parameter Settings. The default path for saving training loss is optimizer_params[\"summary_dir\"], and the weight parameters are saved in the ckpt folder. The model is stacked with multiple encoders and decoders, using the MSELoss loss function and the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bbef106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare params\n",
    "config = load_yaml_config(args.config_file_path)\n",
    "data_params = config[\"transformer_data\"]\n",
    "model_params = config[\"transformer\"]\n",
    "optimizer_params = config[\"transformer_optimizer\"]\n",
    "\n",
    "# prepare summary file\n",
    "summary_dir = optimizer_params[\"summary_dir\"]\n",
    "ckpt_dir = os.path.join(summary_dir, \"ckpt\")\n",
    "\n",
    "# prepare model\n",
    "model = Informer(**model_params)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = nn.Adam(\n",
    "    model.trainable_params(),\n",
    "    optimizer_params[\"lr\"],\n",
    "    weight_decay=optimizer_params[\"weight_decay\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5a08fc",
   "metadata": {},
   "source": [
    "## Transformer dataset loading and processing\n",
    "\n",
    "The Transformer network dataset is generated by CAE's encoder to create the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6e5aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "latent_true = cae_prediction(args.config_file_path)\n",
    "dataset, _ = create_transformer_dataset(\n",
    "    latent_true,\n",
    "    data_params[\"batch_size\"],\n",
    "    data_params[\"time_size\"],\n",
    "    data_params[\"latent_size\"],\n",
    "    data_params[\"time_window\"],\n",
    "    data_params[\"gaussian_filter_sigma\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5ec56",
   "metadata": {},
   "source": [
    "## Transformer training\n",
    "\n",
    "The construction of loss function network, the creation of model checkpoint callback function and the initialization of trainer begin the training of Transformer network and visualize the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b97708",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "time_now = time.time()\n",
    "loss_net = CustomWithLossCell(model, loss_fn, data_params)\n",
    "config = CheckpointConfig(save_checkpoint_steps=50, keep_checkpoint_max=2)\n",
    "ckpt_callback = ModelCheckpoint(\n",
    "     prefix=\"Informer\", directory=ckpt_dir, config=config\n",
    ")\n",
    "trainer = ms.Model(network=loss_net, optimizer=optimizer)\n",
    "for epoch in range(optimizer_params[\"epochs\"]):\n",
    "    print(f\">>>>>>>>>>>>>>>>>>>>Train_{epoch}<<<<<<<<<<<<<<<<<<<<\")\n",
    "    ts = time.time()\n",
    "    trainer.train(\n",
    "        1,\n",
    "        dataset,\n",
    "        callbacks=[\n",
    "            ckpt_callback,\n",
    "            LossMonitor(optimizer_params[\"lr\"], per_print_times=50),\n",
    "        ],\n",
    "    )\n",
    "    print(\"Train Time Cost:\", time.time() - ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aac646",
   "metadata": {},
   "source": [
    "## Visualization of predicted flow field results\n",
    "\n",
    "Run cae_lstm_prediction.py to view the prediction results of the CAE-LSTM reduced order model\n",
    "\n",
    "The following figures show the actual flow field, the predicted results of the CAE-LSTM network, and the corresponding average relative error. The error of CAE-LSTM prediction results is greater than that of CAE reconstruction because the former has more LSTM evolution error than the latter, but the overall prediction time error remains below 0.02, meeting the accuracy requirements of flow field prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ba22d",
   "metadata": {},
   "source": [
    "<figure class=\"harf\">\n",
    "    <img src=\"./images/true2.gif\" title=\"cae_prediction\" width=\"300\"/>\n",
    "    <img src=\"./images/cae_lstm.gif\" title=\"cae_lstm_prediction\" width=\"300\"/>\n",
    "    <img src=\"./images/cae_lstm_error.png\" title=\"cae_lstm_error\" width=\"300\"/>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
